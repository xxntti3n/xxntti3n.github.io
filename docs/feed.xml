<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://xxntti3n.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xxntti3n.github.io/" rel="alternate" type="text/html" /><updated>2026-02-14T16:57:05+00:00</updated><id>https://xxntti3n.github.io/feed.xml</id><title type="html">xxntti3n</title><subtitle>Technical guides, tutorials, and explorations in software engineering</subtitle><entry><title type="html">Kubernetes RBAC &amp;amp; Service Accounts Deep Dive</title><link href="https://xxntti3n.github.io/kubernetes/rbac-security-deep-dive/" rel="alternate" type="text/html" title="Kubernetes RBAC &amp;amp; Service Accounts Deep Dive" /><published>2025-02-22T00:00:00+00:00</published><updated>2025-02-22T00:00:00+00:00</updated><id>https://xxntti3n.github.io/kubernetes/kubernetes-rbac-security-deep-dive</id><content type="html" xml:base="https://xxntti3n.github.io/kubernetes/rbac-security-deep-dive/"><![CDATA[<p>RBAC (Role-Based Access Control) in Kubernetes controls who can do what on which resources. This deep dive covers ServiceAccounts, Roles and ClusterRoles, bindings, and practical least-privilege patterns with YAML and commands.</p>

<hr />

<h2 id="1-rbac-building-blocks">1. RBAC building blocks</h2>

<ul>
  <li><strong>Who</strong> — User, group, or <strong>ServiceAccount</strong> (for Pods and in-cluster automation).</li>
  <li><strong>What</strong> — <strong>Role</strong> or <strong>ClusterRole</strong> (list of API resources + verbs: get, list, create, update, delete, patch, watch).</li>
  <li><strong>Binding</strong> — <strong>RoleBinding</strong> (grants a Role in one namespace) or <strong>ClusterRoleBinding</strong> (grants a ClusterRole cluster-wide).</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ServiceAccount "app-sa" (in namespace prod)
        │
        │  RoleBinding "app-sa-reader"
        ▼
  Role "pod-reader" (in namespace prod)
        │
        └──► rules: pods get, list, watch
</code></pre></div></div>

<hr />

<h2 id="2-serviceaccount">2. ServiceAccount</h2>

<p>Every Pod has a ServiceAccount (default: <code class="language-plaintext highlighter-rouge">default</code> in the same namespace). The token is mounted so processes in the Pod can call the API server.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceAccount</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">app-sa</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
</code></pre></div></div>

<p>Use a <strong>dedicated</strong> ServiceAccount per app or team so you can scope RBAC to that identity.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In Deployment/Pod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">serviceAccountName</span><span class="pi">:</span> <span class="s">app-sa</span>
  <span class="c1"># optional: automountServiceAccountToken: false</span>
</code></pre></div></div>

<hr />

<h2 id="3-role-namespaced">3. Role (namespaced)</h2>

<p>A <strong>Role</strong> applies to a single namespace: it lists resources and verbs allowed in that namespace.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Role</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-reader</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
  <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">pods"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">pods/log"</span><span class="pi">]</span>
  <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">list"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">watch"</span><span class="pi">]</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
  <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">configmaps"</span><span class="pi">]</span>
  <span class="na">resourceNames</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">app-config"</span><span class="pi">]</span>
  <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">]</span>
</code></pre></div></div>

<ul>
  <li><strong>apiGroups: [””]</strong> — Core group (pods, services, configmaps, secrets, etc.).</li>
  <li><strong>resources</strong> — Plural resource name.</li>
  <li><strong>resourceNames</strong> — Optional; restrict to specific resource names.</li>
  <li><strong>verbs</strong> — get, list, create, update, patch, delete, watch, deletecollection.</li>
</ul>

<hr />

<h2 id="4-rolebinding">4. RoleBinding</h2>

<p>A <strong>RoleBinding</strong> grants a Role (or ClusterRole) to subjects (user, group, or ServiceAccount) <strong>in one namespace</strong>.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">RoleBinding</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">app-sa-reader</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">subjects</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceAccount</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">app-sa</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">roleRef</span><span class="pi">:</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">Role</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-reader</span>
  <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
</code></pre></div></div>

<p>Now Pods using <code class="language-plaintext highlighter-rouge">serviceAccountName: app-sa</code> in <code class="language-plaintext highlighter-rouge">prod</code> can get/list/watch Pods and get the ConfigMap <code class="language-plaintext highlighter-rouge">app-config</code> in <code class="language-plaintext highlighter-rouge">prod</code> only.</p>

<hr />

<h2 id="5-clusterrole-and-clusterrolebinding">5. ClusterRole and ClusterRoleBinding</h2>

<p><strong>ClusterRole</strong> — Can define permissions for:</p>

<ul>
  <li>Namespaced resources (in any or all namespaces), or</li>
  <li>Cluster-scoped resources (nodes, PVs, namespaces, etc.).</li>
</ul>

<p><strong>ClusterRoleBinding</strong> — Grants a ClusterRole to subjects <strong>cluster-wide</strong>.</p>

<p>Example: read-only access to Pods in all namespaces.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-reader-global</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
  <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">pods"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">pods/log"</span><span class="pi">]</span>
  <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">list"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">watch"</span><span class="pi">]</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRoleBinding</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">read-pods-global</span>
<span class="na">subjects</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceAccount</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">monitoring</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">monitoring</span>
<span class="na">roleRef</span><span class="pi">:</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-reader-global</span>
  <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
</code></pre></div></div>

<p><strong>Reusing a ClusterRole in one namespace</strong> — Create a <strong>RoleBinding</strong> (not ClusterRoleBinding) and set <code class="language-plaintext highlighter-rouge">roleRef</code> to the ClusterRole. The ClusterRole is then applied only in the binding’s namespace. Built-in roles like <code class="language-plaintext highlighter-rouge">view</code>, <code class="language-plaintext highlighter-rouge">edit</code>, <code class="language-plaintext highlighter-rouge">admin</code> are ClusterRoles; you bind them per namespace with a RoleBinding.</p>

<hr />

<h2 id="6-least-privilege-examples">6. Least-privilege examples</h2>

<p><strong>CI/CD deployer (one namespace):</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ClusterRole: deploy in a namespace (deployments, pods, services, etc.)</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deployer</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">apps"</span><span class="pi">]</span>
  <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">deployments"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">replicasets"</span><span class="pi">]</span>
  <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">list"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">watch"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">create"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">update"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">patch"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">delete"</span><span class="pi">]</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
  <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">pods"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">services"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">configmaps"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">secrets"</span><span class="pi">]</span>
  <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">list"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">watch"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">create"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">update"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">patch"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">delete"</span><span class="pi">]</span>
<span class="nn">---</span>
<span class="c1"># RoleBinding in namespace "prod"</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">RoleBinding</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cicd-deployer</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">subjects</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceAccount</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cicd</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">cicd</span>
<span class="na">roleRef</span><span class="pi">:</span>
  <span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterRole</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">deployer</span>
  <span class="na">apiGroup</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io</span>
</code></pre></div></div>

<p><strong>App that only reads its own ConfigMap/Secret:</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">rbac.authorization.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Role</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">app-config-reader</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">rules</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
  <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">configmaps"</span><span class="pi">]</span>
  <span class="na">resourceNames</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">app-config"</span><span class="pi">]</span>
  <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">]</span>
<span class="pi">-</span> <span class="na">apiGroups</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">"</span><span class="pi">]</span>
  <span class="na">resources</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">secrets"</span><span class="pi">]</span>
  <span class="na">resourceNames</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">app-secret"</span><span class="pi">]</span>
  <span class="na">verbs</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">get"</span><span class="pi">]</span>
</code></pre></div></div>

<hr />

<h2 id="7-checking-and-debugging">7. Checking and debugging</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># What can a ServiceAccount do in a namespace?</span>
kubectl auth can-i <span class="nt">--list</span> <span class="nt">--as</span><span class="o">=</span>system:serviceaccount:prod:app-sa <span class="nt">-n</span> prod

<span class="c"># Single check</span>
kubectl auth can-i create pods <span class="nt">--as</span><span class="o">=</span>system:serviceaccount:prod:app-sa <span class="nt">-n</span> prod

<span class="c"># Why can't a Pod do something? Check SA, RoleBinding, Role/ClusterRole</span>
kubectl get rolebinding,clusterrolebinding <span class="nt">-A</span> <span class="nt">-o</span> wide | <span class="nb">grep </span>app-sa
kubectl describe rolebinding &lt;name&gt; <span class="nt">-n</span> prod
kubectl get role &lt;role-name&gt; <span class="nt">-n</span> prod <span class="nt">-o</span> yaml
</code></pre></div></div>

<hr />

<h2 id="8-audit-and-security-tips">8. Audit and security tips</h2>

<ul>
  <li>Enable <strong>audit logging</strong> on the API server to log who did what (create/delete/patch).</li>
  <li>Prefer <strong>dedicated ServiceAccounts</strong> per app; avoid using <code class="language-plaintext highlighter-rouge">default</code> for sensitive workloads.</li>
  <li>Prefer <strong>Role + RoleBinding</strong> (namespace-scoped) over ClusterRoleBinding when possible.</li>
  <li>Use <strong>resourceNames</strong> to restrict access to specific ConfigMaps/Secrets.</li>
  <li>Review <strong>ClusterRoleBindings</strong> to <code class="language-plaintext highlighter-rouge">cluster-admin</code> and other powerful ClusterRoles; limit to a few identities.</li>
</ul>

<hr />

<h2 id="summary">Summary</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Use</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ServiceAccount</td>
      <td>Identity for Pods and in-cluster clients</td>
    </tr>
    <tr>
      <td>Role</td>
      <td>Permissions in one namespace</td>
    </tr>
    <tr>
      <td>ClusterRole</td>
      <td>Reusable permission set (namespaced or cluster resources)</td>
    </tr>
    <tr>
      <td>RoleBinding</td>
      <td>Grant Role/ClusterRole in one namespace</td>
    </tr>
    <tr>
      <td>ClusterRoleBinding</td>
      <td>Grant ClusterRole cluster-wide</td>
    </tr>
    <tr>
      <td>Least privilege</td>
      <td>Narrow resources and verbs; use resourceNames where possible</td>
    </tr>
  </tbody>
</table>

<p>For how the API server enforces RBAC, see <a href="/kubernetes/control-plane-deep-dive/">Control Plane Deep Dive</a>. For storing credentials used by Pods, see the main <a href="/kubernetes/kubernetes-guide/">Kubernetes guide</a> (ConfigMap &amp; Secret section).</p>]]></content><author><name>Tien Nguyen</name></author><category term="kubernetes" /><summary type="html"><![CDATA[ServiceAccounts, Roles, ClusterRoles, bindings, and least-privilege patterns. With YAML examples and how to audit and debug access.]]></summary></entry><entry><title type="html">Kubernetes Storage Deep Dive — PV, PVC, StorageClass, CSI</title><link href="https://xxntti3n.github.io/kubernetes/storage-pv-pvc-deep-dive/" rel="alternate" type="text/html" title="Kubernetes Storage Deep Dive — PV, PVC, StorageClass, CSI" /><published>2025-02-22T00:00:00+00:00</published><updated>2025-02-22T00:00:00+00:00</updated><id>https://xxntti3n.github.io/kubernetes/kubernetes-storage-pv-pvc-deep-dive</id><content type="html" xml:base="https://xxntti3n.github.io/kubernetes/storage-pv-pvc-deep-dive/"><![CDATA[<p>Kubernetes storage gives Pods durable volumes that survive Pod restarts. This deep dive covers PersistentVolume (PV), PersistentVolumeClaim (PVC), StorageClass, dynamic provisioning, and how to use them for stateful workloads.</p>

<hr />

<h2 id="1-why-pv-and-pvc">1. Why PV and PVC?</h2>

<p>Containers and Pods are ephemeral; <strong>emptyDir</strong> is lost when the Pod is removed. For databases, logs, or any data that must persist, you use <strong>PersistentVolume (PV)</strong> and <strong>PersistentVolumeClaim (PVC)</strong>.</p>

<ul>
  <li><strong>PV</strong> — Cluster resource representing a piece of storage (e.g. a cloud disk or NFS export).</li>
  <li><strong>PVC</strong> — A request for storage (size, access mode, optional StorageClass). Pods use a PVC by name in <code class="language-plaintext highlighter-rouge">volumes</code> and <code class="language-plaintext highlighter-rouge">volumeMounts</code>.</li>
  <li><strong>Binding</strong> — The control plane binds a PVC to a PV that meets the request (capacity, access mode, StorageClass). One PVC ↔ one PV (1:1).</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Pod
   │  volumes: [ persistentVolumeClaim: claimName: data ]
   ▼
  PVC "data" (request: 10Gi, RWO, storageClassName: fast)
   │
   │  bound to
   ▼
  PV (10Gi, RWO, node affinity, CSI volume)
   │
   ▼
  Actual storage (e.g. cloud disk, NFS)
</code></pre></div></div>

<hr />

<h2 id="2-access-modes">2. Access modes</h2>

<table>
  <thead>
    <tr>
      <th>Mode</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ReadWriteOnce (RWO)</strong></td>
      <td>Single node read-write (typical for block volumes).</td>
    </tr>
    <tr>
      <td><strong>ReadOnlyMany (ROX)</strong></td>
      <td>Multiple nodes read-only (e.g. NFS, read-only PVC).</td>
    </tr>
    <tr>
      <td><strong>ReadWriteMany (RWX)</strong></td>
      <td>Multiple nodes read-write (e.g. NFS, some filers).</td>
    </tr>
  </tbody>
</table>

<p>Many cloud block volumes support only RWO. Check your CSI driver or storage backend.</p>

<hr />

<h2 id="3-static-provisioning-admin-creates-pv">3. Static provisioning: admin creates PV</h2>

<p>An admin creates a PV that points to existing storage (e.g. NFS server path or cloud volume ID). A user creates a PVC; the control plane binds the PVC to a matching PV.</p>

<p><strong>PersistentVolume (admin):</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pv-nfs-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">10Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">persistentVolumeReclaimPolicy</span><span class="pi">:</span> <span class="s">Retain</span>
  <span class="na">nfs</span><span class="pi">:</span>
    <span class="na">server</span><span class="pi">:</span> <span class="s">nfs-server.default.svc.cluster.local</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/exports/data1</span>
</code></pre></div></div>

<p><strong>PersistentVolumeClaim (user/app):</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteMany</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">10Gi</span>
  <span class="c1"># no storageClassName = default StorageClass or match PVs with no class</span>
</code></pre></div></div>

<p><strong>Pod using the PVC:</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">myapp:1.0</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/data</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">data</span>
</code></pre></div></div>

<hr />

<h2 id="4-dynamic-provisioning-storageclass-and-csi">4. Dynamic provisioning: StorageClass and CSI</h2>

<p>With <strong>dynamic provisioning</strong>, you don’t create PVs by hand. You create a <strong>StorageClass</strong> that describes a <em>type</em> of storage (e.g. SSD, cloud provider, CSI driver). When a PVC references that StorageClass (or the default), a <strong>provisioner</strong> (usually a CSI driver) creates the underlying volume and a PV, then binds the PVC to that PV.</p>

<p><strong>StorageClass example (generic):</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fast</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">storageclass.kubernetes.io/is-default-class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">false"</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">pd.csi.storage.gke.io</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">pd-ssd</span>
  <span class="na">replication-type</span><span class="pi">:</span> <span class="s">regional-pd</span>
<span class="na">volumeBindingMode</span><span class="pi">:</span> <span class="s">WaitForFirstConsumer</span>
<span class="na">reclaimPolicy</span><span class="pi">:</span> <span class="s">Delete</span>
</code></pre></div></div>

<ul>
  <li><strong>provisioner</strong> — CSI driver or in-tree provisioner (e.g. <code class="language-plaintext highlighter-rouge">pd.csi.storage.gke.io</code>, <code class="language-plaintext highlighter-rouge">ebs.csi.aws.amazon.com</code>).</li>
  <li><strong>volumeBindingMode: WaitForFirstConsumer</strong> — Create the volume only when a Pod using the PVC is scheduled; then the volume is typically created in the same zone as the node.</li>
  <li><strong>reclaimPolicy</strong> — Delete (delete volume when PVC is deleted) or Retain (keep volume; admin can reuse or delete later).</li>
</ul>

<p><strong>PVC that triggers provisioning:</strong></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">fast</span>
  <span class="na">accessModes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">20Gi</span>
</code></pre></div></div>

<p>No PV is created by the user; the CSI driver creates the disk and the PV, and the PVC is bound automatically.</p>

<hr />

<h2 id="5-reclaim-policy">5. Reclaim policy</h2>

<p>When a <strong>PVC is deleted</strong>:</p>

<ul>
  <li><strong>Retain</strong> — PV remains; status becomes <code class="language-plaintext highlighter-rouge">Released</code>. Data is still on the volume. Admin can manually delete the PV and/or the underlying storage.</li>
  <li><strong>Delete</strong> — The provisioner deletes the underlying storage and the PV (typical for dynamic provisioning).</li>
</ul>

<hr />

<h2 id="6-statefulset-and-storage">6. StatefulSet and storage</h2>

<p><strong>StatefulSets</strong> give Pods stable names (<code class="language-plaintext highlighter-rouge">pod-0</code>, <code class="language-plaintext highlighter-rouge">pod-1</code>, …) and optional <strong>volumeClaimTemplates</strong>. Each replica gets its own PVC (and thus its own PV), so each Pod has its own durable volume.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StatefulSet</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">db</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">serviceName</span><span class="pi">:</span> <span class="s">db</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">db</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">db</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">db</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">postgres:15</span>
        <span class="na">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
          <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/lib/postgresql/data</span>
  <span class="na">volumeClaimTemplates</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">fast</span>
      <span class="na">accessModes</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">ReadWriteOnce"</span><span class="pi">]</span>
      <span class="na">resources</span><span class="pi">:</span>
        <span class="na">requests</span><span class="pi">:</span>
          <span class="na">storage</span><span class="pi">:</span> <span class="s">20Gi</span>
</code></pre></div></div>

<p>Result: <code class="language-plaintext highlighter-rouge">db-0</code> → PVC <code class="language-plaintext highlighter-rouge">data-db-0</code>, <code class="language-plaintext highlighter-rouge">db-1</code> → <code class="language-plaintext highlighter-rouge">data-db-1</code>, etc. Each PVC is bound to its own PV.</p>

<hr />

<h2 id="7-inspecting-and-debugging">7. Inspecting and debugging</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pv
kubectl get pvc <span class="nt">-n</span> prod
kubectl describe pvc data <span class="nt">-n</span> prod
kubectl describe pv &lt;pv-name&gt;
</code></pre></div></div>

<ul>
  <li><strong>PVC Pending</strong> — No PV matches (capacity, access mode, StorageClass) or provisioner failed; check StorageClass and CSI driver logs.</li>
  <li><strong>PV Released</strong> — PVC was deleted; reclaim policy Retain. Reuse by deleting the PV and creating a new PVC, or re-import the PV with a new PVC (advanced).</li>
</ul>

<hr />

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PV</td>
      <td>Cluster resource for a piece of storage</td>
    </tr>
    <tr>
      <td>PVC</td>
      <td>Request for storage; Pods reference the PVC by name</td>
    </tr>
    <tr>
      <td>StorageClass</td>
      <td>Type of storage + provisioner; enables dynamic provisioning</td>
    </tr>
    <tr>
      <td>CSI</td>
      <td>Standard for volume drivers; most clusters use CSI for cloud volumes</td>
    </tr>
    <tr>
      <td>volumeClaimTemplate</td>
      <td>In StatefulSet: one PVC per replica</td>
    </tr>
    <tr>
      <td>reclaimPolicy</td>
      <td>Retain or Delete when PVC is removed</td>
    </tr>
  </tbody>
</table>

<p>For how the control plane binds PVCs to PVs, see <a href="/kubernetes/control-plane-deep-dive/">Control Plane Deep Dive</a>. For the full Kubernetes guide, see <a href="/kubernetes/kubernetes-guide/">Kubernetes guide</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="kubernetes" /><summary type="html"><![CDATA[PersistentVolume and PersistentVolumeClaim lifecycle, StorageClass, provisioning, and CSI drivers. With YAML and stateful workload patterns.]]></summary></entry><entry><title type="html">Kubernetes Deployments &amp;amp; Rolling Updates Deep Dive</title><link href="https://xxntti3n.github.io/kubernetes/deployments-rolling-updates-deep-dive/" rel="alternate" type="text/html" title="Kubernetes Deployments &amp;amp; Rolling Updates Deep Dive" /><published>2025-02-21T00:00:00+00:00</published><updated>2025-02-21T00:00:00+00:00</updated><id>https://xxntti3n.github.io/kubernetes/kubernetes-deployments-rolling-updates-deep-dive</id><content type="html" xml:base="https://xxntti3n.github.io/kubernetes/deployments-rolling-updates-deep-dive/"><![CDATA[<p>Deployments manage ReplicaSets and Pods and drive rolling updates and rollbacks. This deep dive covers strategy tuning, rollout commands, and practical canary/blue-green patterns.</p>

<hr />

<h2 id="1-deployment--desired-state--rollout">1. Deployment = desired state + rollout</h2>

<p>A <strong>Deployment</strong> declares:</p>

<ul>
  <li><strong>Replicas</strong> — How many Pods.</li>
  <li><strong>Pod template</strong> — Image, env, resources, probes (the “what” to run).</li>
  <li><strong>Strategy</strong> — How to replace old Pods with new ones (rolling update or recreate).</li>
</ul>

<p>The controller creates/updates <strong>ReplicaSets</strong> so that the current RS matches the desired replicas and template; the ReplicaSet then creates/deletes <strong>Pods</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Deployment (replicas: 3, image: app:v2)
       │
       ▼
  ReplicaSet (app:v2) ──► Pod, Pod, Pod
       │
       │  (previous ReplicaSet kept for rollback)
       ▼
  ReplicaSet (app:v1) ──► (scaled to 0 after rollout)
</code></pre></div></div>

<hr />

<h2 id="2-rolling-update-strategy">2. Rolling update strategy</h2>

<p><strong>strategy.type: RollingUpdate</strong> (default) replaces Pods gradually. Two knobs:</p>

<ul>
  <li><strong>maxSurge</strong> — How many <em>extra</em> Pods above desired count can exist during the rollout (e.g. 1 or 25%).</li>
  <li><strong>maxUnavailable</strong> — How many Pods can be <em>unavailable</em> during the rollout (e.g. 0 or 25%).</li>
</ul>

<p>Example: 10 replicas, maxSurge=2, maxUnavailable=0 → at most 12 Pods (10 old + 2 new), and you never go below 10 available.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">api</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">10</span>
  <span class="na">strategy</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">RollingUpdate</span>
    <span class="na">rollingUpdate</span><span class="pi">:</span>
      <span class="na">maxSurge</span><span class="pi">:</span> <span class="m">2</span>
      <span class="na">maxUnavailable</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">api</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">api</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">api</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">myreg/api:v2</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">128Mi"</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">100m"</span>
          <span class="na">limits</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">256Mi"</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">500m"</span>
        <span class="na">livenessProbe</span><span class="pi">:</span>
          <span class="na">httpGet</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/health</span>
            <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
          <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">5</span>
          <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">10</span>
        <span class="na">readinessProbe</span><span class="pi">:</span>
          <span class="na">httpGet</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/ready</span>
            <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
          <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">5</span>
</code></pre></div></div>

<ul>
  <li><strong>maxUnavailable: 0</strong> — Good when you must not drop capacity (e.g. low replica count or strict SLA).</li>
  <li><strong>maxSurge: 1</strong> — Slower, fewer extra resources; <strong>maxSurge: 25%</strong> — Faster rollout, more temporary Pods.</li>
</ul>

<hr />

<h2 id="3-recreate-strategy">3. Recreate strategy</h2>

<p><strong>strategy.type: Recreate</strong> — All old Pods are deleted first, then new ones are created. Downtime between old and new. Use only when you need a full stop (e.g. schema migration) or when the app cannot run two versions at once.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">strategy</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">Recreate</span>
</code></pre></div></div>

<hr />

<h2 id="4-rollout-and-rollback-commands">4. Rollout and rollback commands</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Trigger rollout (e.g. after changing image or env)</span>
kubectl <span class="nb">set </span>image deployment/api <span class="nv">api</span><span class="o">=</span>myreg/api:v3 <span class="nt">-n</span> prod
<span class="c"># or</span>
kubectl apply <span class="nt">-f</span> deployment.yaml

<span class="c"># Status</span>
kubectl rollout status deployment/api <span class="nt">-n</span> prod

<span class="c"># Pause / resume (e.g. canary: deploy one, test, then resume)</span>
kubectl rollout pause deployment/api <span class="nt">-n</span> prod
kubectl rollout resume deployment/api <span class="nt">-n</span> prod

<span class="c"># History (ReplicaSets)</span>
kubectl rollout <span class="nb">history </span>deployment/api <span class="nt">-n</span> prod

<span class="c"># Rollback to previous revision</span>
kubectl rollout undo deployment/api <span class="nt">-n</span> prod

<span class="c"># Rollback to specific revision</span>
kubectl rollout undo deployment/api <span class="nt">--to-revision</span><span class="o">=</span>2 <span class="nt">-n</span> prod
</code></pre></div></div>

<p>Rollback is implemented by re-applying a previous ReplicaSet template; the Deployment controller does another rolling update “back” to that template.</p>

<hr />

<h2 id="5-canary-pattern-manual">5. Canary pattern (manual)</h2>

<ul>
  <li><strong>Stable Deployment</strong> — e.g. 9 replicas, <code class="language-plaintext highlighter-rouge">app: api</code>, <code class="language-plaintext highlighter-rouge">version: stable</code>.</li>
  <li><strong>Canary Deployment</strong> — e.g. 1 replica, <code class="language-plaintext highlighter-rouge">app: api</code>, <code class="language-plaintext highlighter-rouge">version: canary</code>.</li>
  <li><strong>Service</strong> — Selector <code class="language-plaintext highlighter-rouge">app: api</code> (no version). Traffic is split 9:1 by replica count.</li>
  <li>Promote canary: scale stable to 0, canary to 10, or make canary the new template and delete canary Deployment.</li>
</ul>

<p>You can also use two Services (e.g. <code class="language-plaintext highlighter-rouge">api-stable</code> and <code class="language-plaintext highlighter-rouge">api-canary</code>) and split traffic at Ingress or mesh level.</p>

<hr />

<h2 id="6-blue-green-manual">6. Blue-green (manual)</h2>

<ul>
  <li><strong>Blue Deployment</strong> — e.g. <code class="language-plaintext highlighter-rouge">api-blue</code>, 10 replicas, image v1.</li>
  <li><strong>Green Deployment</strong> — e.g. <code class="language-plaintext highlighter-rouge">api-green</code>, 10 replicas, image v2.</li>
  <li><strong>Service</strong> — Selector points to one of them (e.g. <code class="language-plaintext highlighter-rouge">version: blue</code>). Switch selector to <code class="language-plaintext highlighter-rouge">version: green</code> to cut over.</li>
</ul>

<p>No gradual rollout; instant switch. Rollback = switch selector back to blue.</p>

<hr />

<h2 id="7-practical-tips">7. Practical tips</h2>

<ul>
  <li><strong>Always set resource requests/limits</strong> on the Deployment template so scheduling and capacity are predictable.</li>
  <li><strong>Use readiness probes</strong> so new Pods get traffic only when ready; otherwise rolling update can hit starting Pods.</li>
  <li><strong>Use rollout status</strong> in CI/CD and consider failing the pipeline if the rollout doesn’t complete in time.</li>
  <li><strong>Revision history</strong> is limited (default 10); adjust with <code class="language-plaintext highlighter-rouge">spec.revisionHistoryLimit</code>.</li>
</ul>

<hr />

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th>Topic</th>
      <th>Takeaway</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Strategy</td>
      <td>RollingUpdate (maxSurge / maxUnavailable) or Recreate</td>
    </tr>
    <tr>
      <td>Rollback</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl rollout undo</code> or <code class="language-plaintext highlighter-rouge">--to-revision=N</code></td>
    </tr>
    <tr>
      <td>Canary</td>
      <td>Separate Deployment with fewer replicas, same Service selector</td>
    </tr>
    <tr>
      <td>Blue-green</td>
      <td>Two Deployments; switch Service selector to cut over</td>
    </tr>
  </tbody>
</table>

<p>For how Pods and probes behave during a rollout, see <a href="/kubernetes/pods-deep-dive/">Pods Deep Dive</a>. For the control plane that drives Deployments, see <a href="/kubernetes/control-plane-deep-dive/">Control Plane Deep Dive</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="kubernetes" /><summary type="html"><![CDATA[Deployment strategy, maxSurge and maxUnavailable, rollback, canary and blue-green patterns. With YAML and kubectl examples.]]></summary></entry><entry><title type="html">Kubernetes Services &amp;amp; Cluster Networking Deep Dive</title><link href="https://xxntti3n.github.io/kubernetes/services-networking-deep-dive/" rel="alternate" type="text/html" title="Kubernetes Services &amp;amp; Cluster Networking Deep Dive" /><published>2025-02-21T00:00:00+00:00</published><updated>2025-02-21T00:00:00+00:00</updated><id>https://xxntti3n.github.io/kubernetes/kubernetes-services-networking-deep-dive</id><content type="html" xml:base="https://xxntti3n.github.io/kubernetes/services-networking-deep-dive/"><![CDATA[<p>Services give Pods a stable name and IP and load-balance across backend Pods. This deep dive covers Service types, kube-proxy modes, CoreDNS, and how to debug and tune cluster networking.</p>

<hr />

<h2 id="1-why-services">1. Why Services?</h2>

<p>Pods are ephemeral: IPs change when Pods are recreated. A <strong>Service</strong> is a stable endpoint (cluster-internal IP or node port) that forwards traffic to a set of Pods selected by labels.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Client (Pod or external)
        │
        ▼
   Service (ClusterIP 10.96.x.x or NodePort 3xxxx)
        │
        ├──► Pod 1 (backend)
        ├──► Pod 2 (backend)
        └──► Pod 3 (backend)
</code></pre></div></div>

<hr />

<h2 id="2-service-types">2. Service types</h2>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Use case</th>
      <th>How to reach</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ClusterIP</strong></td>
      <td>In-cluster traffic only</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;service&gt;.&lt;ns&gt;.svc.cluster.local</code> or cluster IP</td>
    </tr>
    <tr>
      <td><strong>NodePort</strong></td>
      <td>Expose on every node’s IP at a fixed port (30000–32767)</td>
      <td><code class="language-plaintext highlighter-rouge">&lt;NodeIP&gt;:&lt;NodePort&gt;</code></td>
    </tr>
    <tr>
      <td><strong>LoadBalancer</strong></td>
      <td>Cloud LB in front of Service (e.g. AWS NLB, GCP Network LB)</td>
      <td>LB’s external IP/hostname</td>
    </tr>
  </tbody>
</table>

<h3 id="clusterip-default">ClusterIP (default)</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">api</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">prod</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">api</span>
    <span class="na">tier</span><span class="pi">:</span> <span class="s">backend</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>
</code></pre></div></div>

<ul>
  <li><strong>port</strong> — Port the Service listens on (e.g. 80).</li>
  <li><strong>targetPort</strong> — Port on the Pod (e.g. 8080). Omit to use the same as <code class="language-plaintext highlighter-rouge">port</code>.</li>
  <li><strong>selector</strong> — Only Pods with these labels are in the endpoints.</li>
</ul>

<p>DNS: <code class="language-plaintext highlighter-rouge">api.prod.svc.cluster.local</code> (or <code class="language-plaintext highlighter-rouge">api</code> from the same namespace).</p>

<h3 id="nodeport">NodePort</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">web</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">web</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
    <span class="na">nodePort</span><span class="pi">:</span> <span class="m">30080</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
</code></pre></div></div>

<p>Traffic to <strong>any node’s IP:30080</strong> is forwarded to a backend Pod. NodePort is in range 30000–32767 unless you set it explicitly (and the cluster allows it).</p>

<h3 id="loadbalancer">LoadBalancer</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">web</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">web</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
</code></pre></div></div>

<p>On AWS/GCP/Azure, the cloud controller creates a load balancer and sets <code class="language-plaintext highlighter-rouge">status.loadBalancer.ingress[0].ip</code> (or hostname). Traffic goes LB → NodePort/ClusterIP → Pods.</p>

<hr />

<h2 id="3-endpoints-and-endpointslices">3. Endpoints and EndpointSlices</h2>

<p>The control plane creates <strong>Endpoints</strong> (and optionally <strong>EndpointSlices</strong>) that list the Pod IPs and ports matching the Service selector. kube-proxy (and the cloud LB) use this list to forward traffic.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get endpoints &lt;service&gt; <span class="nt">-n</span> &lt;ns&gt;
kubectl get endpointslices <span class="nt">-n</span> &lt;ns&gt;
</code></pre></div></div>

<p>If <strong>Endpoints</strong> is empty, no Pods match the selector—check labels and Pod readiness.</p>

<hr />

<h2 id="4-kube-proxy-modes">4. kube-proxy modes</h2>

<p><strong>kube-proxy</strong> runs on every node and programs <strong>rules</strong> so that traffic to the Service IP or NodePort is forwarded to a backend Pod.</p>

<ul>
  <li><strong>iptables</strong> (default in many clusters) — Rules in iptables NAT table; random backend choice.</li>
  <li><strong>ipvs</strong> — More scalable; supports multiple scheduling algorithms (rr, lc, dh, etc.).</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># See which mode (often in kube-proxy Pod args or ConfigMap)</span>
kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> k8s-app<span class="o">=</span>kube-proxy <span class="nt">-o</span> yaml | <span class="nb">grep</span> <span class="nt">-i</span> mode
</code></pre></div></div>

<hr />

<h2 id="5-coredns-and-service-discovery">5. CoreDNS and service discovery</h2>

<p>CoreDNS resolves names like <code class="language-plaintext highlighter-rouge">api.prod.svc.cluster.local</code> to the Service’s ClusterIP. Short names work within the same namespace: <code class="language-plaintext highlighter-rouge">api</code> → <code class="language-plaintext highlighter-rouge">api.prod.svc.cluster.local</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Test DNS from a Pod</span>
kubectl run <span class="nt">-it</span> <span class="nt">--rm</span> debug <span class="nt">--image</span><span class="o">=</span>busybox:1.36 <span class="nt">--restart</span><span class="o">=</span>Never <span class="nt">--</span> nslookup api.prod
</code></pre></div></div>

<p><strong>Pod DNS:</strong> <code class="language-plaintext highlighter-rouge">&lt;pod-ip&gt;.&lt;namespace&gt;.pod.cluster.local</code> (for headless Services or StatefulSet Pods).</p>

<hr />

<h2 id="6-headless-service">6. Headless Service</h2>

<p>For a <strong>headless</strong> Service, you set <code class="language-plaintext highlighter-rouge">clusterIP: None</code>. No virtual IP is allocated; DNS returns the <strong>Pod IPs</strong> of the backing Pods (or EndpointSlices). Used for StatefulSets or when you need to talk to specific Pods.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">db</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterIP</span><span class="pi">:</span> <span class="s">None</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">db</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">5432</span>
</code></pre></div></div>

<p>DNS for <code class="language-plaintext highlighter-rouge">db.namespace.svc.cluster.local</code> can return multiple A records (one per Pod).</p>

<hr />

<h2 id="7-debugging">7. Debugging</h2>

<table>
  <thead>
    <tr>
      <th>Symptom</th>
      <th>Check</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>No connectivity to Service</td>
      <td><code class="language-plaintext highlighter-rouge">kubectl get endpoints</code> — are backends listed? Readiness probes passing?</td>
    </tr>
    <tr>
      <td>DNS not resolving</td>
      <td>CoreDNS Pods running? <code class="language-plaintext highlighter-rouge">nslookup</code> from a test Pod.</td>
    </tr>
    <tr>
      <td>NodePort not reachable</td>
      <td>Security groups / firewall; <code class="language-plaintext highlighter-rouge">kubectl get svc</code> for nodePort.</td>
    </tr>
    <tr>
      <td>LoadBalancer stuck Pending</td>
      <td>Cloud provider integration; events on the Service.</td>
    </tr>
  </tbody>
</table>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get svc <span class="nt">-n</span> &lt;ns&gt;
kubectl describe svc &lt;name&gt; <span class="nt">-n</span> &lt;ns&gt;
kubectl get endpoints &lt;name&gt; <span class="nt">-n</span> &lt;ns&gt;
</code></pre></div></div>

<hr />

<h2 id="8-summary">8. Summary</h2>

<ul>
  <li><strong>ClusterIP</strong> — Stable in-cluster name and IP; use for app-to-app traffic.</li>
  <li><strong>NodePort</strong> — Fixed port on every node; useful for dev or behind your own LB.</li>
  <li><strong>LoadBalancer</strong> — Cloud LB; standard for exposing HTTP/HTTPS or TCP in production.</li>
  <li><strong>Endpoints</strong> — Built from selector; must be non-empty for traffic to reach Pods.</li>
  <li><strong>CoreDNS</strong> — Resolves <code class="language-plaintext highlighter-rouge">svc.ns.svc.cluster.local</code> and (for headless) Pod IPs.</li>
</ul>

<p>For exposing HTTP/HTTPS with TLS and path routing, see <a href="/kubernetes/kubernetes-guide/#ingress">Ingress</a> in the main guide. For how Pods are selected by Services, see <a href="/kubernetes/pods-deep-dive/">Pods Deep Dive</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="kubernetes" /><summary type="html"><![CDATA[ClusterIP, NodePort, LoadBalancer, kube-proxy, CoreDNS, and service discovery. With YAML, debugging, and production patterns.]]></summary></entry><entry><title type="html">Kubernetes Control Plane Deep Dive — API Server, etcd, Scheduler, Controllers</title><link href="https://xxntti3n.github.io/kubernetes/control-plane-deep-dive/" rel="alternate" type="text/html" title="Kubernetes Control Plane Deep Dive — API Server, etcd, Scheduler, Controllers" /><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><id>https://xxntti3n.github.io/kubernetes/kubernetes-control-plane-deep-dive</id><content type="html" xml:base="https://xxntti3n.github.io/kubernetes/control-plane-deep-dive/"><![CDATA[<p>The Kubernetes control plane is the brain of the cluster. This deep dive walks through each component—API server, etcd, scheduler, and controller manager—with how they interact, how to inspect them, and what to tune in production.</p>

<hr />

<h2 id="control-plane-at-a-glance">Control plane at a glance</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────────────────────────────────┐
│                        CONTROL PLANE COMPONENTS                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   kubectl / kubelet / controllers                                           │
│            │                                                                 │
│            ▼                                                                 │
│   ┌─────────────────┐     ┌─────────┐     ┌──────────────────────────────┐ │
│   │   API Server    │────►│  etcd   │     │  Scheduler                     │ │
│   │   (kube-apiserver)│     │ (state) │     │  (assigns Pods to Nodes)      │ │
│   └────────┬────────┘     └─────────┘     └──────────────────────────────┘ │
│            │                                                                 │
│            ▼                                                                 │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │   Controller Manager (deployment, replica, namespace, PV, ...)       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
</code></pre></div></div>

<p>All cluster changes go through the <strong>API server</strong>. It validates requests, persists state in <strong>etcd</strong>, and triggers the <strong>scheduler</strong> and <strong>controllers</strong> to reconcile the desired state.</p>

<hr />

<h2 id="1-api-server-kube-apiserver">1. API server (kube-apiserver)</h2>

<p>The API server is the only component that talks to etcd. Every <code class="language-plaintext highlighter-rouge">kubectl</code> command, kubelet watch, or controller action is an HTTP request to the API server.</p>

<h3 id="request-flow">Request flow</h3>

<ol>
  <li><strong>Authentication</strong> — Who are you? (client cert, bearer token, or service account)</li>
  <li><strong>Authorization</strong> — Are you allowed to do this? (RBAC, Node, ABAC)</li>
  <li><strong>Admission</strong> — Mutating and validating webhooks (e.g. inject sidecar, enforce policy)</li>
  <li><strong>Schema validation</strong> — Object matches the API version schema</li>
  <li><strong>Persistence</strong> — Write to etcd (for create/update/delete) or read from etcd (for get/list/watch)</li>
</ol>

<h3 id="inspecting-the-api-server">Inspecting the API server</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># API server is typically a static Pod or systemd service</span>
kubectl get pods <span class="nt">-n</span> kube-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>kube-apiserver

<span class="c"># Check which admission plugins are enabled (often visible in manifest or flags)</span>
kubectl get pod <span class="nt">-n</span> kube-system <span class="nt">-l</span> <span class="nv">component</span><span class="o">=</span>kube-apiserver <span class="nt">-o</span> yaml | <span class="nb">grep</span> <span class="nt">-A</span> 50 <span class="s2">"admission"</span>

<span class="c"># Health</span>
kubectl get <span class="nt">--raw</span> /livez
kubectl get <span class="nt">--raw</span> /readyz
</code></pre></div></div>

<h3 id="common-flags-for-ops">Common flags (for ops)</h3>

<table>
  <thead>
    <tr>
      <th>Flag</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--etcd-servers</code></td>
      <td>etcd client URLs</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--service-cluster-ip-range</code></td>
      <td>CIDR for Service ClusterIPs</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--enable-admission-plugins</code></td>
      <td>Which admission controllers to run</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">--audit-log-path</code></td>
      <td>Audit log file for security/compliance</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="2-etcd">2. etcd</h2>

<p>etcd is the key-value store holding all cluster state: Pods, Services, ConfigMaps, Secrets, Deployments, and so on.</p>

<h3 id="data-layout">Data layout</h3>

<p>Keys are hierarchical, for example:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">/registry/pods/&lt;namespace&gt;/&lt;name&gt;</code></li>
  <li><code class="language-plaintext highlighter-rouge">/registry/services/&lt;namespace&gt;/&lt;name&gt;</code></li>
  <li><code class="language-plaintext highlighter-rouge">/registry/deployments/&lt;namespace&gt;/&lt;name&gt;</code></li>
</ul>

<p>Values are serialized API objects (often protobuf).</p>

<h3 id="operations-you-might-run-with-etcdctl">Operations you might run (with etcdctl)</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># If you have etcdctl and certs (e.g. on a control-plane node)</span>
<span class="nb">export </span><span class="nv">ETCDCTL_API</span><span class="o">=</span>3
<span class="nb">export </span><span class="nv">ETCDCTL_ENDPOINTS</span><span class="o">=</span>https://127.0.0.1:2379
<span class="nb">export </span><span class="nv">ETCDCTL_CACERT</span><span class="o">=</span>/etc/kubernetes/pki/etcd/ca.crt
<span class="nb">export </span><span class="nv">ETCDCTL_CERT</span><span class="o">=</span>/etc/kubernetes/pki/etcd/server.crt
<span class="nb">export </span><span class="nv">ETCDCTL_KEY</span><span class="o">=</span>/etc/kubernetes/pki/etcd/server.key

<span class="c"># List keys under registry (read-only inspection)</span>
etcdctl get /registry <span class="nt">--prefix</span> <span class="nt">--keys-only</span> | <span class="nb">head</span> <span class="nt">-20</span>

<span class="c"># Check etcd health</span>
etcdctl endpoint health
</code></pre></div></div>

<h3 id="ha-and-backup">HA and backup</h3>

<ul>
  <li>Run 3 or 5 etcd members (odd for quorum).</li>
  <li>Back up etcd regularly (e.g. <code class="language-plaintext highlighter-rouge">etcdctl snapshot save</code>) and test restore.</li>
  <li>API server uses etcd for all reads/writes; if etcd is down or lost, the cluster cannot function.</li>
</ul>

<hr />

<h2 id="3-scheduler-kube-scheduler">3. Scheduler (kube-scheduler)</h2>

<p>The scheduler assigns <strong>Pods</strong> that have no <code class="language-plaintext highlighter-rouge">nodeName</code> to <strong>Nodes</strong>. It does not start containers; it only writes <code class="language-plaintext highlighter-rouge">nodeName</code> into the Pod spec. The kubelet on that node then runs the Pod.</p>

<h3 id="scheduling-pipeline-simplified">Scheduling pipeline (simplified)</h3>

<ol>
  <li><strong>Filter (predicates)</strong> — Which nodes <em>can</em> run the Pod? (resources, node selector, taints/tolerations, affinity, PV topology)</li>
  <li><strong>Score (priorities)</strong> — Rank feasible nodes (e.g. spread across zones, prefer less loaded nodes)</li>
  <li><strong>Bind</strong> — Write the chosen node into the Pod’s <code class="language-plaintext highlighter-rouge">spec.nodeName</code> via the API server</li>
</ol>

<h3 id="viewing-scheduler-decisions">Viewing scheduler decisions</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Why a Pod is pending</span>
kubectl describe pod &lt;pod&gt; <span class="nt">-n</span> &lt;ns&gt;
<span class="c"># Events will show "Scheduled" or "FailedScheduling" and often the reason (e.g. insufficient CPU)</span>

<span class="c"># Scheduler metrics (if exposed)</span>
<span class="c"># scheduler_binding_duration_seconds, scheduler_scheduling_attempt_duration_seconds, etc.</span>
</code></pre></div></div>

<h3 id="customizing-behavior">Customizing behavior</h3>

<ul>
  <li><strong>Node affinity / anti-affinity</strong> — Prefer or require certain nodes.</li>
  <li><strong>Taints and tolerations</strong> — Reserve nodes for specific workloads; only Pods with matching tolerations get scheduled.</li>
  <li><strong>Pod affinity / anti-affinity</strong> — Co-locate or spread Pods relative to other Pods.</li>
</ul>

<hr />

<h2 id="4-controller-manager-kube-controller-manager">4. Controller manager (kube-controller-manager)</h2>

<p>Controllers watch the API server for changes and try to make the <strong>current state</strong> match the <strong>desired state</strong>. Examples:</p>

<table>
  <thead>
    <tr>
      <th>Controller</th>
      <th>Watches</th>
      <th>Acts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Deployment</td>
      <td>Deployment, ReplicaSet</td>
      <td>Create/update ReplicaSets to match replicas and strategy</td>
    </tr>
    <tr>
      <td>ReplicaSet</td>
      <td>ReplicaSet, Pod</td>
      <td>Create/delete Pods to match replicas</td>
    </tr>
    <tr>
      <td>Node</td>
      <td>Node</td>
      <td>Set conditions, evict Pods when node not ready</td>
    </tr>
    <tr>
      <td>PersistentVolume</td>
      <td>PV, PVC</td>
      <td>Bind PVC to PV; delete PV when PVC is gone (if reclaim policy allows)</td>
    </tr>
    <tr>
      <td>Namespace</td>
      <td>Namespace</td>
      <td>Delete all objects in namespace when namespace is deleted</td>
    </tr>
  </tbody>
</table>

<p>They all follow the same pattern: <strong>watch → diff desired vs current → call API server to create/update/delete</strong>.</p>

<h3 id="where-it-runs">Where it runs</h3>

<p>On many installs, a single process <code class="language-plaintext highlighter-rouge">kube-controller-manager</code> runs multiple controllers in one binary. You can also run controllers out-of-tree (e.g. in separate Pods) as long as they have RBAC and talk to the API server.</p>

<hr />

<h2 id="5-putting-it-together-create-a-deployment">5. Putting it together: create a Deployment</h2>

<ol>
  <li>You run <code class="language-plaintext highlighter-rouge">kubectl apply -f deployment.yaml</code>.</li>
  <li><strong>kubectl</strong> sends a POST to the API server: create this Deployment.</li>
  <li><strong>API server</strong> validates, runs admission, writes the Deployment to <strong>etcd</strong>.</li>
  <li><strong>Deployment controller</strong> (in controller manager) sees the new Deployment, creates a ReplicaSet.</li>
  <li><strong>ReplicaSet controller</strong> sees the new ReplicaSet, creates Pod specs (no <code class="language-plaintext highlighter-rouge">nodeName</code> yet).</li>
  <li><strong>Scheduler</strong> sees Pods with no <code class="language-plaintext highlighter-rouge">nodeName</code>, picks nodes, patches Pods with <code class="language-plaintext highlighter-rouge">nodeName</code>.</li>
  <li><strong>Kubelet</strong> on each node sees Pods assigned to it, pulls images and starts containers.</li>
</ol>

<p>So: <strong>API server + etcd</strong> for state; <strong>scheduler</strong> for placement; <strong>controllers</strong> for creating and updating child objects.</p>

<hr />

<h2 id="6-high-availability-ha">6. High availability (HA)</h2>

<ul>
  <li><strong>API server</strong> — Run multiple replicas behind a load balancer; they are stateless and all talk to the same etcd.</li>
  <li><strong>etcd</strong> — Run 3 or 5 members with a separate cluster; API server points to the etcd client endpoints.</li>
  <li><strong>Scheduler / controller manager</strong> — Run multiple instances with <strong>leader election</strong>; only the leader runs the scheduling/control loops.</li>
</ul>

<hr />

<h2 id="7-debugging-tips">7. Debugging tips</h2>

<ul>
  <li><strong>API server down</strong> — <code class="language-plaintext highlighter-rouge">kubectl</code> and all control-plane traffic fail. Check API server Pods and LB.</li>
  <li><strong>etcd down or degraded</strong> — API server can’t read/write; cluster is effectively down. Check etcd health and quorum.</li>
  <li><strong>Scheduler not scheduling</strong> — Check <code class="language-plaintext highlighter-rouge">kubectl describe pod</code> events; often resource limits, taints, or affinity.</li>
  <li><strong>Controllers not reconciling</strong> — Check controller logs (e.g. <code class="language-plaintext highlighter-rouge">kube-controller-manager</code>); often RBAC or API errors.</li>
</ul>

<p>For day-to-day workload authoring, see <a href="/kubernetes/pods-deep-dive/">Pods Deep Dive</a> and <a href="/kubernetes/deployments-rolling-updates/">Deployments &amp; Rolling Updates</a>. For the big picture, see the <a href="/kubernetes/kubernetes-guide/">Kubernetes guide</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="kubernetes" /><summary type="html"><![CDATA[How the control plane really works: API server request flow, etcd storage, scheduler placement, and controller reconciliation. With flags, HA, and debugging commands.]]></summary></entry><entry><title type="html">Kubernetes Pods Deep Dive — Lifecycle, Probes, Init Containers, Sidecars</title><link href="https://xxntti3n.github.io/kubernetes/pods-deep-dive/" rel="alternate" type="text/html" title="Kubernetes Pods Deep Dive — Lifecycle, Probes, Init Containers, Sidecars" /><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><id>https://xxntti3n.github.io/kubernetes/kubernetes-pods-deep-dive</id><content type="html" xml:base="https://xxntti3n.github.io/kubernetes/pods-deep-dive/"><![CDATA[<p>Pods are the smallest deployable unit in Kubernetes. This deep dive covers the pod lifecycle, health probes, init and sidecar containers, resources, and security context—with concrete YAML and commands.</p>

<hr />

<h2 id="1-pod-lifecycle-states-and-conditions">1. Pod lifecycle (states and conditions)</h2>

<p>A Pod moves through well-defined states:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Created
     │
     ▼
  Pending ──► Running ──► Succeeded / Failed
     │            │
     │            └──► Unknown (node lost)
     │
     └──► (stuck: scheduling / image pull / init)
</code></pre></div></div>

<ul>
  <li><strong>Pending</strong> — Accepted by the API server; not yet scheduled or not yet running (e.g. pulling image, running init containers).</li>
  <li><strong>Running</strong> — At least one container is still running.</li>
  <li><strong>Succeeded</strong> — All containers exited with code 0.</li>
  <li><strong>Failed</strong> — At least one container exited non-zero or was terminated by the system.</li>
  <li><strong>Unknown</strong> — Kubelet could not report status (e.g. node lost).</li>
</ul>

<p>Conditions (in <code class="language-plaintext highlighter-rouge">status.conditions</code>) include <code class="language-plaintext highlighter-rouge">PodScheduled</code>, <code class="language-plaintext highlighter-rouge">Initialized</code>, <code class="language-plaintext highlighter-rouge">ContainersReady</code>, <code class="language-plaintext highlighter-rouge">Ready</code>. Use them to see why a Pod isn’t ready.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pod &lt;pod&gt; <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.conditions}'</span> | jq <span class="nb">.</span>
kubectl describe pod &lt;pod&gt;   <span class="c"># Events + conditions</span>
</code></pre></div></div>

<hr />

<h2 id="2-probes-liveness-readiness-startup">2. Probes: liveness, readiness, startup</h2>

<p>Probes tell Kubernetes whether to <strong>restart</strong> the container (liveness), <strong>send traffic</strong> to it (readiness), and how to handle <strong>slow starters</strong> (startup).</p>

<table>
  <thead>
    <tr>
      <th>Probe</th>
      <th>Fails when…</th>
      <th>Effect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Liveness</strong></td>
      <td>Probe fails</td>
      <td>Container is restarted</td>
    </tr>
    <tr>
      <td><strong>Readiness</strong></td>
      <td>Probe fails</td>
      <td>Pod removed from Service endpoints (no new traffic)</td>
    </tr>
    <tr>
      <td><strong>Startup</strong></td>
      <td>Probe fails</td>
      <td>Container is restarted; while startup is running, liveness is disabled</td>
    </tr>
  </tbody>
</table>

<p>Example: HTTP liveness and readiness, and a startup probe for slow app init.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">app-with-probes</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">web</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">myapp:1.0</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
    <span class="na">startupProbe</span><span class="pi">:</span>
      <span class="na">httpGet</span><span class="pi">:</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/health/startup</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">5</span>
      <span class="na">failureThreshold</span><span class="pi">:</span> <span class="m">30</span>
    <span class="na">livenessProbe</span><span class="pi">:</span>
      <span class="na">httpGet</span><span class="pi">:</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/health/live</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">10</span>
      <span class="na">failureThreshold</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">readinessProbe</span><span class="pi">:</span>
      <span class="na">httpGet</span><span class="pi">:</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/health/ready</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">periodSeconds</span><span class="pi">:</span> <span class="m">5</span>
      <span class="na">failureThreshold</span><span class="pi">:</span> <span class="m">2</span>
</code></pre></div></div>

<ul>
  <li><strong>startupProbe</strong> — Give the app up to 30×5s = 150s to become ready; until it succeeds, liveness is not run.</li>
  <li><strong>livenessProbe</strong> — If <code class="language-plaintext highlighter-rouge">/health/live</code> fails 3 times in a row, the container is restarted.</li>
  <li><strong>readinessProbe</strong> — If it fails, the Pod is taken out of Service until it passes again.</li>
</ul>

<p>Use <strong>exec</strong>, <strong>httpGet</strong>, or <strong>tcpSocket</strong> depending on what your app supports. Avoid using the same endpoint for both liveness and readiness if one is heavier than the other.</p>

<hr />

<h2 id="3-init-containers">3. Init containers</h2>

<p>Init containers run to completion <strong>in order</strong> before the main containers start. Typical uses: wait for a dependency, migrate DB, or copy files into a shared volume.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">initContainers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">wait-db</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">busybox:1.36</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">sh'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">-c'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">until</span><span class="nv"> </span><span class="s">nc</span><span class="nv"> </span><span class="s">-z</span><span class="nv"> </span><span class="s">db</span><span class="nv"> </span><span class="s">5432;</span><span class="nv"> </span><span class="s">do</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">2;</span><span class="nv"> </span><span class="s">done'</span><span class="pi">]</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">migrate</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">myapp:migrate</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s1">'</span><span class="s">rake'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">db:migrate'</span><span class="pi">]</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/data</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">myapp:1.0</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/data</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">data</span>
    <span class="na">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<p>If any init container fails, the Pod is retried (according to <code class="language-plaintext highlighter-rouge">restartPolicy</code>). Init containers always run to completion before the main containers start.</p>

<hr />

<h2 id="4-sidecar-pattern-multi-container-pod">4. Sidecar pattern (multi-container Pod)</h2>

<p>Containers in the same Pod share the same network namespace (same IP, localhost), and can share volumes. A common pattern is a <strong>main app</strong> plus a <strong>sidecar</strong> (e.g. log shipper, proxy, or metrics agent).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────────────┐
│  Pod (one IP, shared volumes)                            │
│  ┌─────────────────┐  ┌─────────────────┐              │
│  │  Main app       │  │  Log shipper     │              │
│  │  writes to      │  │  reads from      │              │
│  │  /var/log/app   │  │  /var/log/app    │              │
│  └────────┬────────┘  └────────┬────────┘              │
│           │                    │                         │
│           └────── volume ─────┘                         │
└─────────────────────────────────────────────────────────┘
</code></pre></div></div>

<p>Example: app plus a sidecar that tails logs and sends them to a log backend.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">myapp:1.0</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">logs</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/log/app</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">log-sidecar</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">log-shipper:1.0</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">logs</span>
      <span class="na">mountPath</span><span class="pi">:</span> <span class="s">/var/log/app</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">logs</span>
    <span class="na">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
</code></pre></div></div>

<p>The sidecar starts and stops with the Pod; use readiness on the main container so traffic only goes to the Pod when the app is ready, not the sidecar.</p>

<hr />

<h2 id="5-resources-requests-and-limits">5. Resources: requests and limits</h2>

<ul>
  <li><strong>requests</strong> — Used for <strong>scheduling</strong>. The scheduler only places the Pod on nodes that have at least that much free CPU/memory.</li>
  <li><strong>limits</strong> — <strong>Enforced on the node</strong>. CPU is throttled when exceeded; memory over limit can get the container OOMKilled.</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">myapp:1.0</span>
    <span class="na">resources</span><span class="pi">:</span>
      <span class="na">requests</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">128Mi"</span>
        <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">100m"</span>
      <span class="na">limits</span><span class="pi">:</span>
        <span class="na">memory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">256Mi"</span>
        <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">500m"</span>
</code></pre></div></div>

<p>Always set <strong>requests</strong> (and limits where you want hard caps). Without requests, the Pod can be scheduled on overloaded nodes; without limits, one Pod can starve others.</p>

<hr />

<h2 id="6-security-context">6. Security context</h2>

<p>You can set security options at <strong>Pod</strong> or <strong>container</strong> level. Container settings override Pod-level for that container.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">securityContext</span><span class="pi">:</span>
    <span class="na">runAsNonRoot</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">runAsUser</span><span class="pi">:</span> <span class="m">1000</span>
    <span class="na">fsGroup</span><span class="pi">:</span> <span class="m">2000</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">myapp:1.0</span>
    <span class="na">securityContext</span><span class="pi">:</span>
      <span class="na">allowPrivilegeEscalation</span><span class="pi">:</span> <span class="no">false</span>
      <span class="na">readOnlyRootFilesystem</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">capabilities</span><span class="pi">:</span>
        <span class="na">drop</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">ALL</span>
</code></pre></div></div>

<ul>
  <li><strong>runAsNonRoot / runAsUser</strong> — Don’t run as root; use a specific UID.</li>
  <li><strong>readOnlyRootFilesystem</strong> — Container root filesystem is read-only (use emptyDir or volumes for writable paths).</li>
  <li><strong>capabilities.drop: ALL</strong> — Drop all Linux capabilities (good default for app containers).</li>
</ul>

<hr />

<h2 id="7-useful-commands">7. Useful commands</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Pod status and reasons</span>
kubectl get pods <span class="nt">-o</span> wide
kubectl describe pod &lt;pod&gt; <span class="nt">-n</span> &lt;ns&gt;

<span class="c"># Logs (main container)</span>
kubectl logs &lt;pod&gt; <span class="nt">-n</span> &lt;ns&gt;

<span class="c"># Logs (specific container in a multi-container Pod)</span>
kubectl logs &lt;pod&gt; <span class="nt">-c</span> &lt;container&gt; <span class="nt">-n</span> &lt;ns&gt;

<span class="c"># Previous container log (after crash/restart)</span>
kubectl logs &lt;pod&gt; <span class="nt">--previous</span> <span class="nt">-n</span> &lt;ns&gt;

<span class="c"># Execute in Pod</span>
kubectl <span class="nb">exec</span> <span class="nt">-it</span> &lt;pod&gt; <span class="nt">-n</span> &lt;ns&gt; <span class="nt">--</span> /bin/sh
kubectl <span class="nb">exec</span> &lt;pod&gt; <span class="nt">-c</span> sidecar <span class="nt">-n</span> &lt;ns&gt; <span class="nt">--</span> /bin/sh
</code></pre></div></div>

<hr />

<h2 id="summary">Summary</h2>

<table>
  <thead>
    <tr>
      <th>Topic</th>
      <th>Takeaway</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Lifecycle</td>
      <td>Pending → Running → Succeeded/Failed; use <code class="language-plaintext highlighter-rouge">describe</code> and <code class="language-plaintext highlighter-rouge">conditions</code> when stuck</td>
    </tr>
    <tr>
      <td>Probes</td>
      <td>Liveness = restart; readiness = remove from Service; startup = protect slow start</td>
    </tr>
    <tr>
      <td>Init</td>
      <td>Run before main containers; use for dependencies or setup</td>
    </tr>
    <tr>
      <td>Sidecar</td>
      <td>Same Pod = same network + shared volumes; good for logs, proxy, metrics</td>
    </tr>
    <tr>
      <td>Resources</td>
      <td>Set requests (scheduling) and limits (enforcement)</td>
    </tr>
    <tr>
      <td>Security</td>
      <td>Prefer runAsNonRoot, readOnlyRootFilesystem, drop capabilities</td>
    </tr>
  </tbody>
</table>

<p>For how Pods are created and updated by controllers, see <a href="/kubernetes/deployments-rolling-updates/">Deployments &amp; Rolling Updates</a>. For how they get traffic, see <a href="/kubernetes/services-networking-deep-dive/">Services &amp; Cluster Networking</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="kubernetes" /><summary type="html"><![CDATA[Pod lifecycle, liveness/readiness/startup probes, init containers, sidecar pattern, resource limits, and security context. With YAML and kubectl examples.]]></summary></entry><entry><title type="html">Iceberg Compaction and Maintenance — Deep Dive</title><link href="https://xxntti3n.github.io/iceberg/compaction-and-maintenance/" rel="alternate" type="text/html" title="Iceberg Compaction and Maintenance — Deep Dive" /><published>2025-02-18T00:00:00+00:00</published><updated>2025-02-18T00:00:00+00:00</updated><id>https://xxntti3n.github.io/iceberg/iceberg-compaction-and-maintenance</id><content type="html" xml:base="https://xxntti3n.github.io/iceberg/compaction-and-maintenance/"><![CDATA[<p>Keeping Iceberg tables healthy means expiring old snapshots, rewriting manifests and data files when needed, and removing orphan files. This post covers what each operation does and when to run it, with concrete examples.</p>

<hr />

<h2 id="1-why-maintenance-matters">1. Why maintenance matters</h2>

<ul>
  <li><strong>Snapshots</strong> accumulate with every commit; without expiration, metadata grows and time-travel history can become huge.</li>
  <li><strong>Manifests</strong> can become many small files; rewriting them improves planning and pruning.</li>
  <li><strong>Data files</strong> can become small and numerous (e.g. after many small writes or merge-on-read); rewriting consolidates them and can apply delete files so future reads are cheaper.</li>
  <li><strong>Orphan files</strong> (leftover from failed jobs or old snapshots) waste storage and should be removed with care.</li>
</ul>

<hr />

<h2 id="2-expire-snapshots">2. Expire snapshots</h2>

<p><strong>What it does</strong>: Removes snapshot entries from metadata older than a given time (or beyond a retention policy). Data files that are <strong>only</strong> referenced by expired snapshots become candidates for physical deletion (e.g. by a separate orphan file cleanup).</p>

<p><strong>Example (Spark procedure style):</strong></p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CALL</span> <span class="k">catalog</span><span class="p">.</span><span class="k">system</span><span class="p">.</span><span class="n">expire_snapshots</span><span class="p">(</span>
  <span class="k">table</span> <span class="o">=&gt;</span> <span class="s1">'db.orders'</span><span class="p">,</span>
  <span class="n">older_than</span> <span class="o">=&gt;</span> <span class="nb">TIMESTAMP</span> <span class="s1">'2024-01-01 00:00:00'</span>
<span class="p">);</span>
</code></pre></div></div>

<p><strong>Example (Java API):</strong></p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">table</span><span class="o">.</span><span class="na">expireSnapshots</span><span class="o">()</span>
     <span class="o">.</span><span class="na">expireOlderThan</span><span class="o">(</span><span class="nc">System</span><span class="o">.</span><span class="na">currentTimeMillis</span><span class="o">()</span> <span class="o">-</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">1000L</span><span class="o">)</span>
     <span class="o">.</span><span class="na">commit</span><span class="o">();</span>
</code></pre></div></div>

<p><strong>Best practice</strong>: Run regularly (e.g. daily) with a retention window that matches your time-travel and compliance needs (e.g. 7 days).</p>

<hr />

<h2 id="3-rewrite-manifests">3. Rewrite manifests</h2>

<p><strong>What it does</strong>: Combines small manifest files into fewer, larger manifests. Improves planning and partition pruning because the engine reads fewer, better-organized manifest files.</p>

<p><strong>When</strong>: After many small commits (e.g. streaming) when you have a large number of manifest files.</p>

<p><strong>Example (Spark procedure):</strong></p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CALL</span> <span class="k">catalog</span><span class="p">.</span><span class="k">system</span><span class="p">.</span><span class="n">rewrite_manifests</span><span class="p">(</span><span class="s1">'db.orders'</span><span class="p">);</span>
</code></pre></div></div>

<hr />

<h2 id="4-rewrite-data-files-compaction">4. Rewrite data files (compaction)</h2>

<p><strong>What it does</strong>: Reads a set of data files (and their delete files), merges them into fewer, larger data files, and optionally applies delete files so the new files have no delete files (merge-on-read → copy-on-write for those rows). Reduces small-file count and read-time merge cost.</p>

<p><strong>When</strong>: After many small writes or when merge-on-read has accumulated many delete files.</p>

<p><strong>Example (Spark procedure):</strong></p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CALL</span> <span class="k">catalog</span><span class="p">.</span><span class="k">system</span><span class="p">.</span><span class="n">rewrite_data_files</span><span class="p">(</span>
  <span class="k">table</span> <span class="o">=&gt;</span> <span class="s1">'db.orders'</span><span class="p">,</span>
  <span class="k">where</span> <span class="o">=&gt;</span> <span class="s1">'partition = 20240101'</span>
<span class="p">);</span>
</code></pre></div></div>

<p>You can limit scope by partition or run full-table rewrite; the procedure picks files to combine based on size/count.</p>

<hr />

<h2 id="5-remove-old-metadata-files">5. Remove old metadata files</h2>

<p>Each commit creates a new metadata file. Old metadata files can be removed once no longer needed for history. Some catalogs or table properties support “delete previous metadata after commit” (e.g. keep last N). Untracked or very old metadata files can be removed by <strong>orphan file deletion</strong> (see below) if your tool supports it and you’re careful not to delete the current one.</p>

<hr />

<h2 id="6-delete-orphan-files">6. Delete orphan files</h2>

<p><strong>What it does</strong>: Deletes files in the table directory that are not referenced by any valid snapshot or metadata (e.g. leftover from failed writes or expired snapshots).</p>

<p><strong>When</strong>: Run after expiring snapshots and with a safety horizon (e.g. only delete files older than 24 hours) to avoid removing in-progress writes.</p>

<p><strong>Example (Spark procedure):</strong></p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CALL</span> <span class="k">catalog</span><span class="p">.</span><span class="k">system</span><span class="p">.</span><span class="n">remove_orphan_files</span><span class="p">(</span>
  <span class="k">table</span> <span class="o">=&gt;</span> <span class="s1">'db.orders'</span><span class="p">,</span>
  <span class="n">older_than</span> <span class="o">=&gt;</span> <span class="nb">TIMESTAMP</span> <span class="s1">'2024-01-15 00:00:00'</span>
<span class="p">);</span>
</code></pre></div></div>

<p><strong>Caution</strong>: Ensure no concurrent jobs are still writing; use a conservative <code class="language-plaintext highlighter-rouge">older_than</code> so you don’t delete active files.</p>

<hr />

<h2 id="7-suggested-maintenance-schedule">7. Suggested maintenance schedule</h2>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>Frequency</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Expire snapshots</td>
      <td>Daily</td>
      <td>Limit history and free unreferenced data</td>
    </tr>
    <tr>
      <td>Rewrite manifests</td>
      <td>Weekly or as needed</td>
      <td>Reduce manifest count</td>
    </tr>
    <tr>
      <td>Rewrite data files</td>
      <td>Weekly or as needed</td>
      <td>Consolidate small files, apply deletes</td>
    </tr>
    <tr>
      <td>Remove orphan files</td>
      <td>After expire_snapshots</td>
      <td>Clean storage</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th>Operation</th>
      <th>Effect</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Expire snapshots</td>
      <td>Drop old versions from metadata; allow file cleanup</td>
    </tr>
    <tr>
      <td>Rewrite manifests</td>
      <td>Fewer, larger manifest files</td>
    </tr>
    <tr>
      <td>Rewrite data files</td>
      <td>Fewer, larger data files; apply delete files</td>
    </tr>
    <tr>
      <td>Remove orphan files</td>
      <td>Delete unreferenced files in table location</td>
    </tr>
  </tbody>
</table>

<p>For how snapshots and expiration interact with time travel, see <a href="/iceberg/snapshots-time-travel/">Snapshots and Time Travel</a>. For how row-level deletes and merge-on-read work, see <a href="/iceberg/row-level-updates-deletes/">Row-Level Updates and Deletes</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="iceberg" /><summary type="html"><![CDATA[Expire snapshots, rewrite manifests, rewrite data files, and delete orphan files. Keep metadata and storage healthy with examples.]]></summary></entry><entry><title type="html">Iceberg Row-Level Updates and Deletes — Deep Dive</title><link href="https://xxntti3n.github.io/iceberg/row-level-updates-deletes/" rel="alternate" type="text/html" title="Iceberg Row-Level Updates and Deletes — Deep Dive" /><published>2025-02-18T00:00:00+00:00</published><updated>2025-02-18T00:00:00+00:00</updated><id>https://xxntti3n.github.io/iceberg/iceberg-row-level-updates-deletes</id><content type="html" xml:base="https://xxntti3n.github.io/iceberg/row-level-updates-deletes/"><![CDATA[<p>Apache Iceberg supports row-level UPDATE and DELETE. Under the hood, deletes are represented by <strong>delete files</strong> (position deletes or equality deletes), and the engine can either <strong>rewrite data files</strong> (copy-on-write) or <strong>apply deletes at read time</strong> (merge-on-read). This deep dive explains both strategies and the delete file types, with examples.</p>

<hr />

<h2 id="1-two-strategies-copy-on-write-vs-merge-on-read">1. Two strategies: copy-on-write vs merge-on-read</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────────────────────────┐
│  COPY-ON-WRITE (CoW)                                                 │
│  • Rewrite entire data files with deletes applied                     │
│  • No delete files left; reads are simple                             │
│  • Higher write I/O, lower read cost                                  │
├─────────────────────────────────────────────────────────────────────┤
│  MERGE-ON-READ (MoR)                                                 │
│  • Write delete files (position or equality); apply at read time     │
│  • Lower write I/O, higher read cost (merge pass)                     │
│  • Compaction can later rewrite files to remove delete files          │
└─────────────────────────────────────────────────────────────────────┘
</code></pre></div></div>

<p><em>Suggested image: CoW = one data file in, one new data file out; MoR = data file + delete file, reader merges.</em></p>

<hr />

<h2 id="2-delete-file-types">2. Delete file types</h2>

<h3 id="21-position-deletes">2.1 Position deletes</h3>

<p>Each record is a <strong>(file path, row position)</strong>. “Remove row at position N in file F.”</p>

<ul>
  <li><strong>Pros</strong>: Simple, works for any row; no need to know key columns.</li>
  <li><strong>Cons</strong>: Tied to a specific data file; if the file is compacted or rewritten, positions change (so position deletes are usually rewritten too).</li>
</ul>

<p>Format (conceptual): e.g. Parquet with <code class="language-plaintext highlighter-rouge">file_path</code>, <code class="language-plaintext highlighter-rouge">pos</code>, and optionally full row.</p>

<h3 id="22-equality-deletes">2.2 Equality deletes</h3>

<p>Each record is a <strong>row key</strong> (one or more column values). “Remove any row where (col1, col2) = (v1, v2).”</p>

<ul>
  <li><strong>Pros</strong>: Independent of file layout; good for key-based deletes (e.g. primary key).</li>
  <li><strong>Cons</strong>: Reader must apply by joining/key lookup; need to choose the right columns (often identifier columns).</li>
</ul>

<p>Format: Parquet with the same columns as the equality delete spec (e.g. <code class="language-plaintext highlighter-rouge">id</code>).</p>

<hr />

<h2 id="3-when-to-use-which-strategy">3. When to use which strategy</h2>

<table>
  <thead>
    <tr>
      <th>Scenario</th>
      <th>Prefer</th>
      <th>Reason</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Delete &lt; 20–30% of rows</td>
      <td>Merge-on-read</td>
      <td>Avoid rewriting whole files</td>
    </tr>
    <tr>
      <td>Delete &gt; 20–30% of rows</td>
      <td>Copy-on-write</td>
      <td>Rewrite cost similar to long-term read cost</td>
    </tr>
    <tr>
      <td>Read-heavy, few updates</td>
      <td>Copy-on-write</td>
      <td>No read-time merge</td>
    </tr>
    <tr>
      <td>Write-heavy / streaming</td>
      <td>Merge-on-read</td>
      <td>Lower write latency; compact later</td>
    </tr>
    <tr>
      <td>Key-based DELETE/UPDATE</td>
      <td>Equality deletes</td>
      <td>Natural fit for identifier columns</td>
    </tr>
  </tbody>
</table>

<p>Engines choose position vs equality (and CoW vs MoR) based on table properties and the kind of operation (e.g. “DELETE WHERE id = 5” → equality delete on <code class="language-plaintext highlighter-rouge">id</code> if supported).</p>

<hr />

<h2 id="4-how-update-is-implemented">4. How UPDATE is implemented</h2>

<p>UPDATE is <strong>DELETE + INSERT</strong>: mark matching rows as deleted (via delete files or CoW rewrite), then write new rows. So the same delete semantics (position or equality) and strategies (CoW vs MoR) apply.</p>

<hr />

<h2 id="5-example-sql-and-spark">5. Example: SQL and Spark</h2>

<p><strong>DELETE (conceptual):</strong></p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">DELETE</span> <span class="k">FROM</span> <span class="n">db</span><span class="p">.</span><span class="n">orders</span> <span class="k">WHERE</span> <span class="n">status</span> <span class="o">=</span> <span class="s1">'cancelled'</span><span class="p">;</span>
<span class="c1">-- Engine may produce equality deletes on status or position deletes</span>
</code></pre></div></div>

<p><strong>UPDATE (conceptual):</strong></p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">UPDATE</span> <span class="n">db</span><span class="p">.</span><span class="n">orders</span> <span class="k">SET</span> <span class="n">status</span> <span class="o">=</span> <span class="s1">'shipped'</span> <span class="k">WHERE</span> <span class="n">order_id</span> <span class="o">=</span> <span class="mi">12345</span><span class="p">;</span>
<span class="c1">-- Engine: delete row(s) for order_id 12345, insert new row(s)</span>
</code></pre></div></div>

<p>In Spark you can tune behavior via table properties (e.g. prefer copy-on-write vs merge-on-read, or delete file format).</p>

<hr />

<h2 id="6-read-path-with-deletes">6. Read path with deletes</h2>

<ol>
  <li>Read manifest list and manifests for the current snapshot.</li>
  <li>Collect data files and delete files (position and equality) from manifests.</li>
  <li>For each data file:
    <ul>
      <li>Apply <strong>position deletes</strong>: skip rows whose (file_path, pos) is in a position-delete file.</li>
      <li>Apply <strong>equality deletes</strong>: skip rows whose key columns match any row in an equality-delete file.</li>
    </ul>
  </li>
  <li>Return remaining rows.</li>
</ol>

<p>Compaction (rewrite) can merge data files and delete files into new data files with no delete files, simplifying future reads.</p>

<hr />

<h2 id="7-summary">7. Summary</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Copy-on-write</td>
      <td>Rewrite data files; no delete files; simple reads</td>
    </tr>
    <tr>
      <td>Merge-on-read</td>
      <td>Keep delete files; apply at read; compact later</td>
    </tr>
    <tr>
      <td>Position delete</td>
      <td>(file_path, row position)</td>
    </tr>
    <tr>
      <td>Equality delete</td>
      <td>Key column values; good for primary-key deletes</td>
    </tr>
    <tr>
      <td>UPDATE</td>
      <td>DELETE + INSERT</td>
    </tr>
  </tbody>
</table>

<p>For cleaning up delete files and optimizing layout, see <a href="/iceberg/compaction-and-maintenance/">Compaction and Maintenance</a>. For how delete files are stored in the metadata and manifest layer, see <a href="/iceberg/table-metadata-file-layout/">Table Metadata and File Layout</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="iceberg" /><summary type="html"><![CDATA[How Iceberg implements UPDATE and DELETE: copy-on-write vs merge-on-read, position deletes vs equality deletes, and when to use each.]]></summary></entry><entry><title type="html">Iceberg Branches and Tags — Deep Dive</title><link href="https://xxntti3n.github.io/iceberg/branches-and-tags/" rel="alternate" type="text/html" title="Iceberg Branches and Tags — Deep Dive" /><published>2025-02-17T00:00:00+00:00</published><updated>2025-02-17T00:00:00+00:00</updated><id>https://xxntti3n.github.io/iceberg/iceberg-branches-and-tags</id><content type="html" xml:base="https://xxntti3n.github.io/iceberg/branches-and-tags/"><![CDATA[<p>Iceberg supports <strong>branches</strong> and <strong>tags</strong> as named references to snapshots, similar to Git. Branches are mutable (they can move to newer snapshots); tags are immutable (they always point to the same snapshot). This enables parallel development, release pinning, and patterns like Write-Audit-Publish (WAP).</p>

<hr />

<h2 id="1-branches-vs-tags-at-a-glance">1. Branches vs tags at a glance</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────────────────────────┐
│  BRANCH: mutable pointer                                            │
│  • Points to a snapshot now; can be updated to a newer snapshot      │
│  • Has optional retention (e.g. keep min snapshots, max age)        │
│  • Use: main, feature-x, audit-branch                               │
├─────────────────────────────────────────────────────────────────────┤
│  TAG: immutable pointer                                             │
│  • Always points to the same snapshot                                │
│  • Use: v1.0.0, release-2024-01-01                                  │
└─────────────────────────────────────────────────────────────────────┘
</code></pre></div></div>

<p><em>Suggested image: snapshot timeline with branch “main” moving and tag “v1.0” fixed.</em></p>

<hr />

<h2 id="2-how-they-are-stored">2. How they are stored</h2>

<p>References live in the table metadata under <strong>refs</strong>:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">"refs"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nl">"main"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"snapshot-id"</span><span class="p">:</span><span class="w"> </span><span class="mi">106</span><span class="p">,</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"branch"</span><span class="p">,</span><span class="w"> </span><span class="nl">"min-snapshots-to-keep"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">},</span><span class="w">
  </span><span class="nl">"v1.0.0"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"snapshot-id"</span><span class="p">:</span><span class="w"> </span><span class="mi">102</span><span class="p">,</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"tag"</span><span class="w"> </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>The catalog (or the table’s metadata location) is the source of truth; engines resolve a branch or tag name to a snapshot-id, then load that snapshot’s manifest list and data.</p>

<hr />

<h2 id="3-typical-use-cases">3. Typical use cases</h2>

<ul>
  <li><strong>main branch</strong>: Default branch; most queries and jobs read from <code class="language-plaintext highlighter-rouge">main</code>.</li>
  <li><strong>Feature / dev branch</strong>: Write and validate changes without affecting <code class="language-plaintext highlighter-rouge">main</code>; merge or promote when ready.</li>
  <li><strong>Tags</strong>: Mark releases or critical points (e.g. <code class="language-plaintext highlighter-rouge">release-2024-01-15</code>) for auditing or rollback.</li>
  <li><strong>WAP (Write-Audit-Publish)</strong>: Write to a branch, run checks, then update <code class="language-plaintext highlighter-rouge">main</code> to that snapshot (publish).</li>
</ul>

<hr />

<h2 id="4-operations-conceptual">4. Operations (conceptual)</h2>

<ul>
  <li><strong>Create branch</strong>: Register a new ref pointing to a snapshot (or current).</li>
  <li><strong>Create tag</strong>: Register an immutable ref pointing to a snapshot.</li>
  <li><strong>Use branch for reads</strong>: Query table <code class="language-plaintext highlighter-rouge">FOR BRANCH branch_name</code> (or engine-specific option).</li>
  <li><strong>Use branch for writes</strong>: Some catalogs support committing to a branch so the branch pointer advances.</li>
  <li><strong>Update main to a snapshot</strong>: Effectively “publish” by moving <code class="language-plaintext highlighter-rouge">main</code> to the snapshot that was written on another branch.</li>
</ul>

<p>Syntax is engine- and catalog-specific (e.g. Spark procedures, Flink, or REST catalog). Check your engine’s docs for <code class="language-plaintext highlighter-rouge">CREATE BRANCH</code>, <code class="language-plaintext highlighter-rouge">CREATE TAG</code>, and how to read/write by branch.</p>

<hr />

<h2 id="5-retention-branches">5. Retention (branches)</h2>

<p>Branch refs can specify retention so that when the branch moves forward, old snapshots can be expired (e.g. keep at least N snapshots, or expire older than T). That keeps metadata and storage under control while still allowing time travel on the branch within the retention window.</p>

<hr />

<h2 id="6-summary">6. Summary</h2>

<table>
  <thead>
    <tr>
      <th>Ref type</th>
      <th>Mutable?</th>
      <th>Typical use</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Branch</td>
      <td>Yes</td>
      <td>main, feature branches, WAP</td>
    </tr>
    <tr>
      <td>Tag</td>
      <td>No</td>
      <td>Releases, audit points</td>
    </tr>
  </tbody>
</table>

<p>For how snapshots and time travel work under the hood, see <a href="/iceberg/snapshots-time-travel/">Snapshots and Time Travel</a>. For the role of the catalog in resolving table and refs, see <a href="/iceberg/iceberg-catalog/">Understanding Apache Iceberg Catalogs</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="iceberg" /><summary type="html"><![CDATA[Git-like branches and tags in Iceberg: mutable vs immutable refs, WAP, parallel development, and how they are stored in metadata.]]></summary></entry><entry><title type="html">Iceberg Partitioning — Hidden Partitioning and Evolution</title><link href="https://xxntti3n.github.io/iceberg/partitioning/" rel="alternate" type="text/html" title="Iceberg Partitioning — Hidden Partitioning and Evolution" /><published>2025-02-17T00:00:00+00:00</published><updated>2025-02-17T00:00:00+00:00</updated><id>https://xxntti3n.github.io/iceberg/iceberg-partitioning</id><content type="html" xml:base="https://xxntti3n.github.io/iceberg/partitioning/"><![CDATA[<p>Iceberg partitioning speeds up queries by grouping rows into files by partition value. Unlike Hive, Iceberg uses <strong>hidden partitioning</strong>: you define transforms (e.g. day(ts)) in the table spec; writers and readers don’t manage partition columns by hand, and you can <strong>evolve</strong> the partition spec without rewriting the whole table.</p>

<hr />

<h2 id="1-why-hidden-partitioning">1. Why “hidden” partitioning?</h2>

<p>In Hive-style tables you must:</p>

<ul>
  <li>Add an explicit partition column (e.g. <code class="language-plaintext highlighter-rouge">event_date</code>) and populate it correctly on write.</li>
  <li>Remember to filter on that column in every query; otherwise you get full scans.</li>
</ul>

<p>In Iceberg:</p>

<ul>
  <li>You partition by <strong>transforms</strong> on columns (e.g. <code class="language-plaintext highlighter-rouge">day(event_time)</code>). There is no separate partition column in the row.</li>
  <li>The engine derives partition values from the data and records them in metadata.</li>
  <li>The engine <strong>translates</strong> predicates (e.g. <code class="language-plaintext highlighter-rouge">event_time BETWEEN ...</code>) into partition filters and skips files automatically.</li>
</ul>

<p>So: no manual partition column, no silent bugs from wrong format or missing filters, and partition layout can change over time (partition evolution).</p>

<hr />

<h2 id="2-partition-transforms">2. Partition transforms</h2>

<p>Common transforms:</p>

<table>
  <thead>
    <tr>
      <th>Transform</th>
      <th>Source type</th>
      <th>Example partition value</th>
      <th>Use case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">identity</code></td>
      <td>any</td>
      <td>same as column</td>
      <td>Category, region</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">year(ts)</code></td>
      <td>timestamp/date</td>
      <td><code class="language-plaintext highlighter-rouge">2024</code></td>
      <td>Coarse time</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">month(ts)</code></td>
      <td>timestamp/date</td>
      <td><code class="language-plaintext highlighter-rouge">2024-01</code></td>
      <td>Monthly rollups</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">day(ts)</code></td>
      <td>timestamp/date</td>
      <td><code class="language-plaintext highlighter-rouge">2024-01-15</code></td>
      <td>Daily tables</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">hour(ts)</code></td>
      <td>timestamp</td>
      <td><code class="language-plaintext highlighter-rouge">2024-01-15T10</code></td>
      <td>Event streams</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">bucket(n, col)</code></td>
      <td>any</td>
      <td><code class="language-plaintext highlighter-rouge">0..n-1</code></td>
      <td>Skew reduction</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">truncate(l, col)</code></td>
      <td>string/int</td>
      <td>truncated value</td>
      <td>Prefix grouping</td>
    </tr>
  </tbody>
</table>

<p>Example: partition by day of <code class="language-plaintext highlighter-rouge">event_time</code> and by <code class="language-plaintext highlighter-rouge">level</code>:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">logs</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="n">event_time</span> <span class="nb">TIMESTAMP</span><span class="p">,</span>
  <span class="k">level</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">message</span> <span class="n">STRING</span>
<span class="p">)</span>
<span class="k">USING</span> <span class="n">iceberg</span>
<span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">days</span><span class="p">(</span><span class="n">event_time</span><span class="p">),</span> <span class="k">level</span><span class="p">);</span>
</code></pre></div></div>

<p>Writes only need <code class="language-plaintext highlighter-rouge">event_time</code> and <code class="language-plaintext highlighter-rouge">level</code>; partition values are derived. Queries like <code class="language-plaintext highlighter-rouge">WHERE event_time BETWEEN '2024-01-01' AND '2024-01-07'</code> are used to prune partitions automatically.</p>

<p><em>Suggested image: diagram of table layout with data/date=2024-01-01/level=ERROR/ and files inside.</em></p>

<hr />

<h2 id="3-partition-spec-in-metadata">3. Partition spec in metadata</h2>

<p>The table’s partition spec is stored in metadata (with a partition-spec-id). Each data file’s manifest entry includes partition values for that spec. When you <strong>evolve</strong> the partition spec (e.g. add a new partition field), new writes use the new spec; old data keeps its old spec. No full table rewrite required.</p>

<hr />

<h2 id="4-partition-evolution-example">4. Partition evolution example</h2>

<p>Start with daily partitioning:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Initial: partition by day</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">events</span> <span class="p">(</span><span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span> <span class="n">ts</span> <span class="nb">TIMESTAMP</span><span class="p">,</span> <span class="n">name</span> <span class="n">STRING</span><span class="p">)</span>
<span class="k">USING</span> <span class="n">iceberg</span>
<span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">days</span><span class="p">(</span><span class="n">ts</span><span class="p">));</span>
</code></pre></div></div>

<p>Later, add monthly partitioning (e.g. for a new rollup). In Spark 3.x with Iceberg:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">events</span> <span class="k">ADD</span> <span class="k">PARTITION</span> <span class="n">FIELD</span> <span class="n">months</span><span class="p">(</span><span class="n">ts</span><span class="p">);</span>
</code></pre></div></div>

<p>New data will be written with both <code class="language-plaintext highlighter-rouge">day(ts)</code> and <code class="language-plaintext highlighter-rouge">month(ts)</code>; existing data stays day-only. Queries can still filter on <code class="language-plaintext highlighter-rouge">ts</code>; the engine uses whatever partition info exists per file.</p>

<hr />

<h2 id="5-best-practices">5. Best practices</h2>

<ul>
  <li><strong>Time-based</strong>: Prefer <code class="language-plaintext highlighter-rouge">days(ts)</code> or <code class="language-plaintext highlighter-rouge">hours(ts)</code> for event/log tables so range queries prune well.</li>
  <li><strong>High cardinality</strong>: Add <code class="language-plaintext highlighter-rouge">bucket(n, id)</code> (or similar) to avoid too many small partitions.</li>
  <li><strong>Evolution</strong>: Prefer adding new partition fields over changing existing ones so old data remains valid.</li>
</ul>

<hr />

<h2 id="6-summary">6. Summary</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Iceberg behavior</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Partition value</td>
      <td>Derived by transform; not a physical column</td>
    </tr>
    <tr>
      <td>Predicate pushdown</td>
      <td>Engine maps column predicates to partition pruning</td>
    </tr>
    <tr>
      <td>Evolution</td>
      <td>New spec for new writes; old data unchanged</td>
    </tr>
  </tbody>
</table>

<p>For how these partitions are stored in manifests and data paths, see <a href="/iceberg/table-metadata-file-layout/">Table Metadata and File Layout</a>. For versioning and rollback of table state, see <a href="/iceberg/snapshots-time-travel/">Snapshots and Time Travel</a>.</p>]]></content><author><name>Tien Nguyen</name></author><category term="iceberg" /><summary type="html"><![CDATA[How Iceberg partitioning works: hidden partitioning, partition specs, transforms, and partition evolution. With SQL and Spark examples.]]></summary></entry></feed>